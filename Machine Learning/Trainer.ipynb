{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43812eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T20:10:26.490086Z",
     "start_time": "2023-10-07T20:10:21.605525Z"
    }
   },
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Input, LeakyReLU, Dropout, BatchNormalization, TimeDistributed\n",
    "from tensorflow.keras.layers import Conv2DTranspose, ConvLSTM2D, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "import cv2\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm as log_progress\n",
    "\n",
    "DATA_PATH = 'C:/Users/lukec/Videos/Test.mkv'\n",
    "\n",
    "TEMP_PATH = 'temp/'\n",
    "if os.path.isdir(TEMP_PATH):\n",
    "    shutil.rmtree(TEMP_PATH)\n",
    "os.mkdir(TEMP_PATH)\n",
    "\n",
    "DATASET_PATH = 'Dataset/'\n",
    "\n",
    "VIDEO_WIDTH = 128\n",
    "VIDEO_HEIGHT = 128\n",
    "FPS = 30 #recorded fps of the input data\n",
    "INPUT_FRAME_COUNT = 60 #about 2 seconds of audio\n",
    "\n",
    "GEN_MORE_SAMPLES = False #USE ONLY IF YOU HAVE A TON OF MEMORY AND PROCESSING POWER!\n",
    "                         #(creates more training samples by offsetting \n",
    "                         #each training point by one frame instead of INPUT_FRAME_COUNT)\n",
    "                         #This means that INPUT_FRAME_COUNT times more training samples will be generated\n",
    "            \n",
    "if GEN_MORE_SAMPLES:\n",
    "    DATASET_PATH = 'Large Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a35ba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T19:49:33.944515Z",
     "start_time": "2023-10-07T19:48:57.455214Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipping audio...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e7eaeecf284ddbb8c578181f54276b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting audio to spectrograms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84112bfa8a847cfaa401162e8360fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(61, 128, 128, 1)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#prep data\n",
    "\n",
    "#strip the audio from the training video using ffmpeg\n",
    "command = f\"ffmpeg -i {DATA_PATH} -ab 160k -ac 1 -ar 44100 -vn {TEMP_PATH}audio.wav\"\n",
    "subprocess.call(command, shell=True)\n",
    "\n",
    "#load video frames from training video and dump them to a file\n",
    "vidcap = cv2.VideoCapture(DATA_PATH)\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "success = True\n",
    "frames = []\n",
    "while success:\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = np.reshape(image, (image.shape[0], image.shape[1], 1))\n",
    "    frames.append(image)\n",
    "    success,image = vidcap.read()\n",
    "    count += 1\n",
    "frames = np.array(frames)\n",
    "frames = frames/255\n",
    "np.save(f'{DATASET_PATH}videoframes.npy', frames)\n",
    "\n",
    "#load audio from file, split into segments the same length as the input frames, save to temp folder\n",
    "audio = AudioSegment.from_wav(f\"{TEMP_PATH}audio.wav\")\n",
    "audio_seg_len = (INPUT_FRAME_COUNT/FPS)\n",
    "\n",
    "num_audio_clips = int((audio.duration_seconds-audio_seg_len) / audio_seg_len)\n",
    "\n",
    "print(\"Clipping audio...\")\n",
    "for i in log_progress(range(num_audio_clips)):\n",
    "    if GEN_MORE_SAMPLES:\n",
    "        for j in range(INPUT_FRAME_COUNT):\n",
    "            t1 = (i * audio_seg_len) + j*(1/FPS) #Works in milliseconds\n",
    "            t2 = t1 + audio_seg_len\n",
    "\n",
    "            t1*=1000\n",
    "            t2*=1000\n",
    "            newAudio = audio[t1:t2]\n",
    "            newAudio.export(f'{TEMP_PATH}{i}_{j}.wav', format=\"wav\")\n",
    "    else:\n",
    "        t1 = i * audio_seg_len #Works in milliseconds\n",
    "        t2 = t1 + audio_seg_len\n",
    "\n",
    "        t1*=1000\n",
    "        t2*=1000\n",
    "        newAudio = audio[t1:t2]\n",
    "        newAudio.export(f'{TEMP_PATH}{i}.wav', format=\"wav\")\n",
    "\n",
    "print(\"Converting audio to spectrograms...\")\n",
    "\n",
    "#run arss on all of those audio files\n",
    "if os.path.isdir(\"cache/\"):\n",
    "    shutil.rmtree(\"cache/\")\n",
    "os.mkdir(\"cache/\")\n",
    "running = True\n",
    "\n",
    "def arssThread(num, second_num=-1):\n",
    "    if second_num==-1:\n",
    "        command = f\"arss {TEMP_PATH}{num}.wav cache/temp_{num}.png -q -min 27 -max 19912 -p {int(VIDEO_WIDTH/(INPUT_FRAME_COUNT/FPS))} -y {VIDEO_HEIGHT}\"\n",
    "        subprocess.call(command, shell=True)\n",
    "    else:\n",
    "        command = f\"arss {TEMP_PATH}{num}_{second_num}.wav cache/temp_{num}.png -q -min 27 -max 19912 -p {int(VIDEO_WIDTH/(INPUT_FRAME_COUNT/FPS))} -y {VIDEO_HEIGHT}\"\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "def arssThreadManager():\n",
    "    global running\n",
    "    \n",
    "    if GEN_MORE_SAMPLES:\n",
    "        for i in log_progress(range(num_audio_clips)):\n",
    "            for j in range(INPUT_FRAME_COUNT):\n",
    "                while len(threads) >= 70:\n",
    "                    for t_id, t in enumerate(threads):\n",
    "                        if not t.is_alive():\n",
    "                            t.join()\n",
    "                            threads.pop(t_id)\n",
    "                    time.sleep(0.1)\n",
    "                x = threading.Thread(target=arssThread, args=(i,j))\n",
    "                x.start()\n",
    "                threads.append(x)\n",
    "    else:\n",
    "        for i in log_progress(range(num_audio_clips)):\n",
    "            while len(threads) >= 20:\n",
    "                for t_id, t in enumerate(threads):\n",
    "                    if not t.is_alive():\n",
    "                        t.join()\n",
    "                        threads.pop(t_id)\n",
    "                time.sleep(0.1)\n",
    "            x = threading.Thread(target=arssThread, args=(i,))\n",
    "            x.start()\n",
    "            threads.append(x)\n",
    "        \n",
    "    while len(threads) > 0:\n",
    "        for t_id, t in enumerate(threads):\n",
    "            if not t.is_alive():\n",
    "                t.join()\n",
    "                threads.pop(t_id)\n",
    "        time.sleep(0.1)\n",
    "    running = False\n",
    "    \n",
    "    \n",
    "spects = [] #spectrogram array\n",
    "threads = []\n",
    "\n",
    "manager = threading.Thread(target=arssThreadManager)\n",
    "manager.start()\n",
    "\n",
    "while running:\n",
    "    for file in glob.glob('cache/*.png'):\n",
    "        try:\n",
    "            #load temp image and save it to array\n",
    "            img = cv2.imread(file)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            img = np.reshape(img, (img.shape[0], img.shape[1], 1))\n",
    "            spects.append(img)\n",
    "            os.remove(file)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "    \n",
    "#clean up\n",
    "manager.join()\n",
    "shutil.rmtree('cache/')\n",
    "\n",
    "spects = np.array(spects)\n",
    "spects = spects/255\n",
    "np.save(f'{DATASET_PATH}spectrograms.npy', spects)\n",
    "print(spects.shape)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccaaaedb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T20:02:01.234005Z",
     "start_time": "2023-10-07T20:02:00.539760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3770, 128, 128, 1)\n",
      "(61, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "#load pre-compiled data\n",
    "frames = np.load(f'{DATASET_PATH}videoframes.npy')\n",
    "spects = np.load(f'{DATASET_PATH}spectrograms.npy')\n",
    "\n",
    "print(frames.shape)\n",
    "print(spects.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d4e7a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T20:10:37.658662Z",
     "start_time": "2023-10-07T20:10:26.493672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d (ConvLSTM2D)    (None, 60, 64, 64, 256)   12896256  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 60, 64, 64, 256)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 60, 64, 64, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (None, 32, 32, 256)       18875392  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 128, 128, 128)     819328    \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 128, 128, 1)       3201      \n",
      "=================================================================\n",
      "Total params: 32,596,225\n",
      "Trainable params: 32,595,201\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#make model (version 1)\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(256, input_shape=(INPUT_FRAME_COUNT, VIDEO_WIDTH, VIDEO_HEIGHT, 1), kernel_size=(7,7), strides=(2,2), padding='same', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(ConvLSTM2D(256, kernel_size=(6,6), strides=(2,2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2DTranspose(128, kernel_size=(5,5), strides=(4,4), padding='same'))\n",
    "model.add(Conv2D(1, kernel_size=(5,5), padding='same'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76561628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T20:02:20.589546Z",
     "start_time": "2023-10-07T20:02:20.434469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 60, 128, 128, 1)\n",
      "(61, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "#prep data\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "if GEN_MORE_SAMPLES:\n",
    "    for i in range(len(frames)):\n",
    "        X.append(frames[i:i+INPUT_FRAME_COUNT])\n",
    "        Y.append(spects[i])\n",
    "else:\n",
    "    for i in range(len(spects)):\n",
    "        start_index = i * INPUT_FRAME_COUNT\n",
    "        X.append(frames[start_index:start_index+INPUT_FRAME_COUNT])\n",
    "        Y.append(spects[i])\n",
    "    \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8382c9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-07T20:02:23.217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/61 [..............................] - ETA: 2:44:33 - loss: 0.5244"
     ]
    }
   ],
   "source": [
    "#training\n",
    "hist = model.fit(X, Y, epochs=1, batch_size=1, verbose=1)\n",
    "model.save(\"v1.keras\")\n",
    "plt.plot(hist.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c47b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
