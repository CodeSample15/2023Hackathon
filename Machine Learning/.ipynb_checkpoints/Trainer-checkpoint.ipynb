{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43812eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T23:26:49.231427Z",
     "start_time": "2023-10-07T23:26:42.700470Z"
    }
   },
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Input, LeakyReLU, Dropout, BatchNormalization, TimeDistributed\n",
    "from tensorflow.keras.layers import Conv2DTranspose, ConvLSTM2D, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "import cv2\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm as log_progress\n",
    "\n",
    "DATA_PATHS = glob.glob('Videos/*.mkv')\n",
    "\n",
    "TEMP_PATH = 'temp/'\n",
    "if os.path.isdir(TEMP_PATH):\n",
    "    shutil.rmtree(TEMP_PATH)\n",
    "os.mkdir(TEMP_PATH)\n",
    "\n",
    "DATASET_PATH = 'Dataset/'\n",
    "\n",
    "VIDEO_WIDTH = 128\n",
    "VIDEO_HEIGHT = 128\n",
    "FPS = 30 #recorded fps of the input data\n",
    "INPUT_FRAME_COUNT = 60 #about 2 seconds of audio\n",
    "\n",
    "GEN_MORE_SAMPLES = False #USE ONLY IF YOU HAVE A TON OF MEMORY AND PROCESSING POWER!\n",
    "                         #(creates more training samples by offsetting \n",
    "                         #each training point by one frame instead of INPUT_FRAME_COUNT)\n",
    "                         #This means that INPUT_FRAME_COUNT times more training samples will be generated\n",
    "            \n",
    "if GEN_MORE_SAMPLES:\n",
    "    DATASET_PATH = 'Large Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a35ba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T23:26:51.230956Z",
     "start_time": "2023-10-07T23:26:49.235829Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.4-dev) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b21986f13cfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.4-dev) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "#prep data\n",
    "\n",
    "#strip the audio from the training video using ffmpeg\n",
    "for i, path in enumerate(DATA_PATHS):\n",
    "    command = f\"ffmpeg -i {path} -ab 160k -ac 1 -ar 44100 -vn {TEMP_PATH}audio_{i}.wav\"\n",
    "    subprocess.call(command, shell=True)\n",
    "\n",
    "#load video frames from training video and dump them to a file\n",
    "frames = []\n",
    "for path in DATA_PATHS:\n",
    "    vidcap = cv2.VideoCapture(path)\n",
    "    success,image = vidcap.read()\n",
    "    success = True\n",
    "    while success:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = np.reshape(image, (image.shape[0], image.shape[1], 1))\n",
    "        frames.append(image)\n",
    "        success,image = vidcap.read()\n",
    "frames = np.array(frames)\n",
    "frames = frames/255\n",
    "np.save(f'{DATASET_PATH}videoframes.npy', frames)\n",
    "\n",
    "#load audio from file, split into segments the same length as the input frames, save to temp folder\n",
    "print(\"Clipping audio...\")\n",
    "\n",
    "audio_seg_len = (INPUT_FRAME_COUNT/FPS)\n",
    "counter = 0\n",
    "audio_clip_count = 0\n",
    "for path in range(len(DATA_PATHS)):\n",
    "    audio = AudioSegment.from_wav(f\"{TEMP_PATH}audio_{path}.wav\")\n",
    "\n",
    "    num_audio_clips = int((audio.duration_seconds-audio_seg_len) / audio_seg_len)\n",
    "\n",
    "    for i in log_progress(range(num_audio_clips)):\n",
    "        if GEN_MORE_SAMPLES:\n",
    "            for j in range(INPUT_FRAME_COUNT):\n",
    "                t1 = (i * audio_seg_len) + j*(1/FPS) #Works in milliseconds\n",
    "                t2 = t1 + audio_seg_len\n",
    "\n",
    "                t1*=1000\n",
    "                t2*=1000\n",
    "                newAudio = audio[t1:t2]\n",
    "                newAudio.export(f'{TEMP_PATH}{counter}_{j}.wav', format=\"wav\")\n",
    "                counter += 1\n",
    "        else:\n",
    "            t1 = i * audio_seg_len #Works in milliseconds\n",
    "            t2 = t1 + audio_seg_len\n",
    "\n",
    "            t1*=1000\n",
    "            t2*=1000\n",
    "            newAudio = audio[t1:t2]\n",
    "            newAudio.export(f'{TEMP_PATH}{counter}.wav', format=\"wav\")\n",
    "            counter += 1\n",
    "            \n",
    "    audio_clip_count += num_audio_clips\n",
    "\n",
    "print(\"Converting audio to spectrograms...\")\n",
    "\n",
    "#run arss on all of those audio files\n",
    "if os.path.isdir(\"cache/\"):\n",
    "    shutil.rmtree(\"cache/\")\n",
    "os.mkdir(\"cache/\")\n",
    "running = True\n",
    "\n",
    "queue = []\n",
    "spects = [] #spectrogram array\n",
    "threads = []\n",
    "\n",
    "def arssThread(num, second_num=-1):\n",
    "    if second_num==-1:\n",
    "        command = f\"arss {TEMP_PATH}{num}.wav cache/temp_{num}.png -q -min 27 -max 19912 -p {int(VIDEO_WIDTH/(INPUT_FRAME_COUNT/FPS))} -y {VIDEO_HEIGHT}\"\n",
    "        subprocess.call(command, shell=True)\n",
    "    else:\n",
    "        command = f\"arss {TEMP_PATH}{num}_{second_num}.wav cache/temp_{num}_{second_num}.png -q -min 27 -max 19912 -p {int(VIDEO_WIDTH/(INPUT_FRAME_COUNT/FPS))} -y {VIDEO_HEIGHT}\"\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "def arssThreadManager():\n",
    "    global running, queue, threads\n",
    "    \n",
    "    if GEN_MORE_SAMPLES:\n",
    "        for i in log_progress(range(audio_clip_count)):\n",
    "            for j in range(INPUT_FRAME_COUNT):\n",
    "                while len(threads) >= 70:\n",
    "                    for t_id, t in enumerate(threads):\n",
    "                        if not t.is_alive():\n",
    "                            t.join()\n",
    "                            threads.pop(t_id)\n",
    "                            \n",
    "                    time.sleep(0.1)\n",
    "                x = threading.Thread(target=arssThread, args=(i,j))\n",
    "                x.start()\n",
    "                queue.append((i, j))\n",
    "                threads.append(x)\n",
    "    else:\n",
    "        for i in log_progress(range(audio_clip_count)):\n",
    "            while len(threads) >= 25:\n",
    "                for t_id, t in enumerate(threads):\n",
    "                    if not t.is_alive():\n",
    "                        t.join()\n",
    "                        threads.pop(t_id)\n",
    "                time.sleep(0.1)\n",
    "            x = threading.Thread(target=arssThread, args=(i,))\n",
    "            x.start()\n",
    "            queue.append(i)\n",
    "            threads.append(x)\n",
    "        \n",
    "    while len(threads) > 0:\n",
    "        for t_id, t in enumerate(threads):\n",
    "            if not t.is_alive():\n",
    "                t.join()\n",
    "                threads.pop(t_id)\n",
    "        time.sleep(0.1)\n",
    "    running = False\n",
    "\n",
    "manager = threading.Thread(target=arssThreadManager)\n",
    "manager.start()\n",
    "\n",
    "def load_spectrogram_from_queue():\n",
    "    global queue, spects\n",
    "    try:\n",
    "        #load temp image and save it to array\n",
    "        file = ''\n",
    "        if GEN_MORE_SAMPLES:\n",
    "            file = f'cache/temp_{queue[0][0]}_{second_num[0][1]}.png'\n",
    "        else:\n",
    "            file = f'cache/temp_{queue[0]}.png'\n",
    "            \n",
    "        img = cv2.imread(file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        img = np.reshape(img, (img.shape[0], img.shape[1], 1))\n",
    "        spects.append(img)\n",
    "        os.remove(file)\n",
    "        \n",
    "        queue.pop(0)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "while running:\n",
    "    load_spectrogram_from_queue()\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "while len(queue) > 0:\n",
    "    load_spectrogram_from_queue()\n",
    "    \n",
    "#clean up\n",
    "manager.join()\n",
    "shutil.rmtree('cache/')\n",
    "shutil.rmtree(TEMP_PATH)\n",
    "\n",
    "spects = np.array(spects)\n",
    "spects = spects/255\n",
    "np.save(f'{DATASET_PATH}spectrograms.npy', spects)\n",
    "print(spects.shape)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccaaaedb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T23:19:00.807225Z",
     "start_time": "2023-10-07T23:19:00.183331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4038, 128, 128, 1)\n",
      "(64, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "#load pre-compiled data\n",
    "frames = np.load(f'{DATASET_PATH}videoframes.npy')\n",
    "spects = np.load(f'{DATASET_PATH}spectrograms.npy')\n",
    "\n",
    "print(frames.shape)\n",
    "print(spects.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d4e7a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T20:10:37.658662Z",
     "start_time": "2023-10-07T20:10:26.493672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d (ConvLSTM2D)    (None, 60, 64, 64, 256)   12896256  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 60, 64, 64, 256)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 60, 64, 64, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (None, 32, 32, 256)       18875392  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 128, 128, 128)     819328    \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 128, 128, 1)       3201      \n",
      "=================================================================\n",
      "Total params: 32,596,225\n",
      "Trainable params: 32,595,201\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#make model (version 1)\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(256, input_shape=(INPUT_FRAME_COUNT, VIDEO_WIDTH, VIDEO_HEIGHT, 1), kernel_size=(7,7), strides=(2,2), padding='same', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(ConvLSTM2D(256, kernel_size=(6,6), strides=(2,2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2DTranspose(128, kernel_size=(5,5), strides=(4,4), padding='same'))\n",
    "model.add(Conv2D(1, kernel_size=(5,5), padding='same'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76561628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T23:19:08.142971Z",
     "start_time": "2023-10-07T23:19:07.892180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 60, 128, 128, 1)\n",
      "(64, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "#prep data\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "if GEN_MORE_SAMPLES:\n",
    "    for i in range(len(frames)):\n",
    "        X.append(frames[i:i+INPUT_FRAME_COUNT])\n",
    "        Y.append(spects[i])\n",
    "else:\n",
    "    for i in range(len(spects)):\n",
    "        start_index = i * INPUT_FRAME_COUNT\n",
    "        X.append(frames[start_index:start_index+INPUT_FRAME_COUNT])\n",
    "        Y.append(spects[i])\n",
    "    \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8382c9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-07T20:02:23.217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/61 [..............................] - ETA: 2:44:33 - loss: 0.5244"
     ]
    }
   ],
   "source": [
    "#training\n",
    "hist = model.fit(X, Y, epochs=1, batch_size=1, verbose=1)\n",
    "model.save(\"v1.keras\")\n",
    "plt.plot(hist.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c47b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
